{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Working with the Azure OpenAI API directly\n",
    "\n",
    "In this lab, we will perform a couple of simple calls to the Azure OpenAI API.\n",
    "- The first call will allow us to find out which Model Deployments are available for the Azure OpenAI API.\n",
    "- The second call will send a prompt to the Azure OpenAI API.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we start by providing the API key for Azure OpenAI and the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set the endpoint that we're going to call. This endpoint will return a list of available model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = RESOURCE_ENDPOINT + \"/openai/deployments?api-version=\" + API_VERSION\n",
    "\n",
    "print (url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you will see the full URL that we are going to be calling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of available Model Deployments\n",
    "\n",
    "Now let's call that Azure OpenAI endpoint and see what it returns. We pass the API key in the HTTP header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, there should now be a JSON formatted list of model deployments. The output will contain one or more sections similar to the following\n",
    "\n",
    "```\n",
    "    {\n",
    "      \"scale_settings\": {\n",
    "        \"scale_type\": \"standard\"\n",
    "      },\n",
    "      \"model\": \"gpt-35-turbo\",\n",
    "      \"owner\": \"organization-owner\",\n",
    "      \"id\": \"gpt35turbo\",\n",
    "      \"status\": \"succeeded\",\n",
    "      \"created_at\": 1684150536,\n",
    "      \"updated_at\": 1684150536,\n",
    "      \"object\": \"deployment\"\n",
    "    }\n",
    "```\n",
    "\n",
    "In the output, you'll see the **Model Name** as the `model` value and the **Deployment Name** as the `id` value. We'll be using an `id` value for the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a prompt to Azure OpenAI using the API\n",
    "\n",
    "Next, let's call the Azure OpenAI API with a prompt. To do this, we'll need the `id` of our completion model deployment.\n",
    "\n",
    "We've already setup this value in the `.env` file, but you should see a matching value in the list that was output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_ID = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll construct a URL to call. This time, we'll also be creating a payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = RESOURCE_ENDPOINT + \"/openai/deployments/\" + DEPLOYMENT_ID + \"/completions?api-version=\" + API_VERSION\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you will see the full URL that we are going to call. This URL will use the specified deployment to call the **completions** API.\n",
    "\n",
    "Next, we will call the Azure OpenAI API using the URL above. Just like last time, we pass the API key in the HTTP header. We also send a JSON formatted body as part of the request which contains the *prompt* that we want to use to get a response from the OpenAI model. In this case, our prompt is \"Once upon a time\", which should cause the model to complete the prompt by generating a story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(url, headers={\"api-key\": API_KEY}, json={\"prompt\": \"Once upon a time\"})\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the result of the API call will be JSON data similar to the below example.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"id\": \"cmpl-7WoRsMTvolWjs2IzK9OdI0MXHHXwu\",\n",
    "  \"object\": \"text_completion\",\n",
    "  \"created\": 1688054776,\n",
    "  \"model\": \"text-davinci-003\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \" there was a lovely princess who have magical powers. She lived in a castle far\",\n",
    "      \"index\": 0,\n",
    "      \"finish_reason\": \"length\",\n",
    "      \"logprobs\": null\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 16,\n",
    "    \"prompt_tokens\": 4,\n",
    "    \"total_tokens\": 20\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Here's some more information about some of the data contained in that response.\n",
    "\n",
    "Key | Description\n",
    "--- | ---\n",
    "`model` | The model that was used to generate the response to the prompt\n",
    "`text` | This is the response that was generated by the OpenAI model\n",
    "`finish_reason` | In this case, the `finish_reason` is `length`, which indicates that the response was cut short due to reaching the *token limit*. We didn't specify a maximum number of tokens in our request, so it will have used the default limit of **16**. Therefore, our response was cut short when the limit of 16 tokens was reached.\n",
    "`completion_tokens` | The number of tokens that were used in generating the response\n",
    "`prompt_tokens` | The number of tokens that were consumed by the prompt\n",
    "`total_tokens` | The total number of tokens that were consumed by the request (`prompt_tokens` + `completion_tokens`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we used the Azure OpenAI API directly to query information about our instance of the Azure OpenAI service and to send a prompt to an OpenAI model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "In the next lab, we will look at using the OpenAI SDK to work with the Azure OpenAI service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
