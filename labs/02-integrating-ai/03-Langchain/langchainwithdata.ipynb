{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Langchain with vector store\n",
    "\n",
    "In this lab, we will introduce [Langchain](https://python.langchain.com/docs/get_started/introduction), a framework for developing applications powered by language models and ask question on custom data using a vector store.\n",
    "\n",
    "Langchain supports Python and Javascript / Typescript. For this lab, we will use Python.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll use the `pip` tool to install the `langchain` Python package and `quadrant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai tiktoken qdrant-client python-dotenv --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the movies csv file the `AzureOpenAI` specific components from the `langchain` package.\n",
    "As with all the other labs, we'll need to provide our API key and endpoint details. We'll also provide the name (id) of the model deployment that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "# API_KEY = \"<YOUR API KEY>\"\n",
    "# RESOURCE_ENDPOINT = \"<YOUR AZURE OPENAI ENDPOINT>\" # For example https://<your azure open ai instance>.openai.azure.com/\n",
    "# DEPLOYMENT_ID = \"<YOUR DEPLOYMENT ID>\" # For example \"text-davinci-003\"\n",
    "load_dotenv()\n",
    "\n",
    "# Set this to `azure`\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data from the csv file into a loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='../../extra/data/movies/movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "data = data[1:200] # reduce dataset if you want\n",
    "print('Loaded %s movies' % len(data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the OpenAI embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=os.environ[\"DEPLOYMENT_ID\"],\n",
    "    model_name=\"gpt-35-turbo\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure Langchain to use Qdrant as vector store using docker, embedd the loaded documents and store the embeddings in the vector store. Depending on the rate limiting this might take a while."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)/labs/extra/data/qdrantstorage:/qdrant/storage\" qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"my_movies\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test the vector store and search for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = qdrant\n",
    "\n",
    "query = \"What is the best 80s movie I should look?\"\n",
    "found_docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to search for similar movies but with a more diverse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Which movies are about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using a QA chain to ask questions about the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(deployment_id=os.environ[\"DEPLOYMENT_ID\"])\n",
    "chain = RetrievalQA.from_chain_type(llm=openai, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\", return_source_documents=True)\n",
    "query = \"Do you have a column called popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])\n",
    "\n",
    "query = \"What is the movie with the highest popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the vector database from a file and ask the same question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del vectorstore\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "qdrantStore = Qdrant(client=client, collection_name=\"my_movies\", embeddings=embeddings)\n",
    "\n",
    "query = \"What are the three best movie about space travel?\"\n",
    "found_docs = qdrantStore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a retriever to query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrantStore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"What are the three best movie about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
