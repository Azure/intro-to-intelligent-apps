{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Langchain with Qdrant\n",
    "\n",
    "In this lab, we will do a deeper dive around the Qdrant vector store and different ways to interact with it.\n",
    "\n",
    "We'll start as usual by defining our Azure OpenAI service API key and endpoint details, specifying the model deployment we want to use and then we'll initiate a connection to the Azure OpenAI service.\n",
    "\n",
    "**NOTE**: As with previous labs, we'll use the values from the `.env` file in the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found OpenAPI Base Endpoint: \" + os.getenv(\"OPENAI_API_BASE\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "completion_model = os.getenv(\"OPENAI_COMPLETION_MODEL\")\n",
    "embedding_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the data from the movies.csv file using a Langchain document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "data = data[1:11] # reduce dataset if you want\n",
    "print('Loaded %s movies' % len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an Azure OpenAI embedding and completion deployments in order to create the vector representation of the movies in the loaded CSV file and then be able to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    deployment=embedding_name,\n",
    "    chunk_size=1\n",
    ") \n",
    "\n",
    "# Create a Chat Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = deployment_name,\n",
    "    model_name=completion_model\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Qdrant Server Locally\n",
    "\n",
    "**NOTE**: Please read the following carefully.\n",
    "\n",
    "If you are running the lab in Codespaces or a VS Code local devcontainer, Qdrant is already running, continue to the next section.\n",
    "\n",
    "Otherwise, we need to start Qdrant and the easiest way is using Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Qdrant Server\n",
    "!docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)/qdrantstorage:/qdrant/storage\" qdrant/qdrant\n",
    "\n",
    "# If you want to stop and cleanup the Qdrant server, uncomment and run the following commands:\n",
    "# !docker stop qdrant\n",
    "# !docker rm qdrant\n",
    "# !rm -rf labs/03-orchestration/03-Qdrant/qdrantstorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movies into Qdrant\n",
    "\n",
    "Now that we have the Qdrant server running and persisting data locally, **see the qdrantstorage directory for more details**, let's load the movies into the vector store.\n",
    "\n",
    "We'll configure Langchain to use Qdrant as vector store, embedd the loaded documents and store the embeddings in the vector store.\n",
    "\n",
    "**NOTE**: Depending on the number of movies loaded and rate limiting, this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"my_movies\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Qdrant\n",
    "\n",
    "Now we are going to test the vector store by searching it in different ways.\n",
    "\n",
    "The first way is to search for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = qdrant\n",
    "\n",
    "query = \"What is the best 80s movie I should look at?\"\n",
    "found_docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to search for similar movies but with more diverse results, note the **mmr** search_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Which movies are about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Langchain Retriever\n",
    "\n",
    "In this part we will use Langchain to search the Qdrant vector store and retrieve the results. This is different than the previous section where we were using Qdrant's search API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using a Langchain QA chain to ask questions about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = deployment_name,\n",
    "    model_name = completion_model\n",
    ")\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\", return_source_documents=True)\n",
    "\n",
    "query = \"Do you have a column called popularity?\"\n",
    "print (\"First query: \" + query)\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print (\"Source documents for first query\")\n",
    "print(response['source_documents'])\n",
    "print (\"\\n\")\n",
    "\n",
    "query = \"What is the movie with the highest popularity?\"\n",
    "print (\"Second query: \" + query)\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(\"Source Documents for second query\")\n",
    "print(response['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "ðŸ“£ [ACS with LC and Python](../04-ACS/acs-lc-python.ipynb)\n",
    "\n",
    "ðŸ“£ [ACS with SK and C#](../04-ACS/acs-sk-csharp.ipynb)\n",
    "\n",
    "ðŸ“£ [ACS with SK and Python](../04-ACS/acs-sk-python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
