{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Langchain with Qdrant\n",
    "\n",
    "In this lab, we will do a deeper dive around the Qdrant vector store and different ways to interact with it.\n",
    "\n",
    "We'll start as usual by defining our Azure OpenAI service API key and endpoint details, specifying the model deployment we want to use and then we'll initiate a connection to the Azure OpenAI service.\n",
    "\n",
    "**NOTE**: As with previous labs, we'll use the values from the `.env` file in the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found OpenAPI Base Endpoint: \" + os.getenv(\"OPENAI_API_BASE\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "embedding_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the data from the movies.csv file using a Langchain document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "data = data[1:11] # reduce dataset if you want\n",
    "print('Loaded %s movies' % len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an Azure OpenAI embedding and completion deployments in order to create the vector representation of the movies in the loaded CSV file and then be able to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    deployment=embedding_name,\n",
    "    chunk_size=1\n",
    ") \n",
    "\n",
    "# Create a Completion Instance of Azure OpenAI\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = deployment_name,\n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Qdrant Server Locally\n",
    "\n",
    "_If you are running the lab in Codespaces, Qdrant is already running, otherwise, we need to start Qdrant and the easiest way is using Docker._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Qdrant Server\n",
    "!docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)/qdrantstorage:/qdrant/storage\" qdrant/qdrant\n",
    "\n",
    "# If you want to stop and cleanup the Qdrant server, uncomment and run the following commands:\n",
    "# !docker stop qdrant\n",
    "# !docker rm qdrant\n",
    "# !rm -rf labs/03-orchestration/03-Qdrant/qdrantstorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure Langchain to use Qdrant as vector store, embedd the loaded documents and store the embeddings in the vector store. Depending on the number of movies loaded and rate limiting, this might take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movies into Qdrant\n",
    "\n",
    "Now that we have the Qdrant server running and persisting data locally, **see the qdrantstorage directory for more details**, let's load the movies into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"my_movies\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Qdrant\n",
    "\n",
    "Now we are going to test the vector store by searching it in different ways.\n",
    "\n",
    "The first way is to search for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = qdrant\n",
    "\n",
    "query = \"What is the best 80s movie I should look?\"\n",
    "found_docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to search for similar movies but with more diverse results, note the **mmr** search_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Which movies are about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Langchain Retriever\n",
    "\n",
    "In this part we will use Langchain to search the Qdrant vector store and retrieve the results. This is different than the previous section where we were using Qdrant's search API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using a Langchain QA chain to ask questions about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = deployment_name,\n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\", return_source_documents=True)\n",
    "query = \"Do you have a column called popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])\n",
    "\n",
    "query = \"What is the movie with the highest popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movies into Qdrant from File\n",
    "\n",
    "Load the vector database from a file and ask the same question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=embedding_name, chunk_size=1) \n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "qdrantStore = Qdrant(client=client, collection_name=\"my_movies\", embeddings=embeddings)\n",
    "\n",
    "query = \"What are the three best movie about space travel?\"\n",
    "found_docs = qdrantStore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets create a retriever to query against the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrantStore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"What are the three best movie about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "ðŸ“£ [ACS](../04-ACS/acs.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
