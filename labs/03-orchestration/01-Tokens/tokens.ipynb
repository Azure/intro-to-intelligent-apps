{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Tokens\n",
    "\n",
    "GPT models process text using *tokens*, which are common sequences of characters found in text. The models understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.\n",
    "\n",
    "The conversion of a prompt into tokens happens automatically when you submit a prompt so you don't need to do anything yourself. However, OpenAI services like Azure OpenAI use the number of tokens processed as part of the pricing model, in the case of Azure OpenAI, charging per 1,000 tokens. So understanding how many tokens your prompts consume is an important part of planning and building any application that will use OpenAI.\n",
    "\n",
    "The prompt **\"Hello world, this is fun!\"** gets tokenized as follows:\n",
    "\n",
    "```\n",
    "Hello\n",
    " world,\n",
    " this\n",
    " is\n",
    " fun\n",
    "!\n",
    "\n",
    "(6 tokens)\n",
    "```\n",
    "Notice how spaces and punctuation are included as part of the tokens. A token doesn't always necessarily equate to a single word or phrase.\n",
    "\n",
    "Let's try the prompt **\"Example using words like indivisible and emojis\"**.\n",
    "\n",
    "```\n",
    "Example\n",
    " using\n",
    " words\n",
    " like\n",
    " ind\n",
    "iv\n",
    "isible\n",
    " and\n",
    " em\n",
    "oj\n",
    "is\n",
    "\n",
    "(11 tokens)\n",
    "```\n",
    "This time you can see that some of the words, **indivisible** and **emojis**, got broken up into smaller chunks.\n",
    "\n",
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\n",
    "\n",
    ":thumbsup: You can experiment with this yourself using the *tokenizer* tool available on the OpenAI website at https://platform.openai.com/tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with tokens in code\n",
    "\n",
    "OpenAI provide the `tiktoken` package that you can use to experiment with tokenization in your code.\n",
    "\n",
    "`tiktoken` supports three encodings used by Azure OpenAI Service models:\n",
    "\n",
    "| Encoding name | Azure OpenAI Service models |\n",
    "| ------------- | -------------- |\n",
    "| gpt2 (or r50k_base) | Most GPT-3 models |\n",
    "| p50k_base | Code models, text-davinci-002, text-davinci-003 |\n",
    "| cl100k_base | text-embedding-ada-002 |\n",
    "\n",
    "You can use `tiktoken` as follows to tokenize a string and see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoding.encode(\"Hello world, this is fun!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the output of the above code what you were expecting?\n",
    "\n",
    "If you were expecting text broken up like the examples at the top of this page, you were probably wondering why you just got back a bunch of seemingly random numbers. This is because the AI models don't work on words. Instead, they use a method called *BPE* (Byte Pair Encoding) to convert the text into numeric tokens.\n",
    "\n",
    "One of the features of BPE is that it's reversible, so you can convert the tokens back into the original text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the text instead of the tokens\n",
    "\n",
    "Check out the following code that displays the human readable text instead of the token values.\n",
    "\n",
    ":bulb: **NOTE:** See the following cookbook for some tips on working with `tiktoken`: [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to display the text from the tokens below\n",
    "[encoding.decode_single_token_bytes(token) for token in encoding.encode(\"Hello world, this is fun!\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see results similar to the following\n",
    "\n",
    "`[b'Hello', b' world', b',', b' this', b' is', b' fun', b'!']`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to return the number of tokens\n",
    "\n",
    "Using what you've learned so far, let's create a function that returns the count of the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens_from_string(string: str, encoding_name: str='p50k_base') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text by a given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "get_num_tokens_from_string(\"Hello World, this is fun!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blowing up the prompt!\n",
    "\n",
    "Apart from cost, there's another reason that you'll want to be in control of the number of tokens you use. All AI models have a limit on the maximum number of tokens that a request can consume. The limit per request includes the number of tokens in the prompt **plus** the number of tokens in the response. Different models can have different token limits, but ultimately the overall size of your prompt and the response to that prompt have to be smaller than that limit.\n",
    "\n",
    "Let's show this in action by deliberately sending a prompt that's too large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a file, `movies.csv`, in this folder which contains a long list of movie data. Let's use `tiktoken` to see how big this file is in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Open the file with information about movies\n",
    "movie_data = os.path.join(os.getcwd(), \"movies.csv\")\n",
    "content = open(movie_data, \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Use tiktoken to tokenize the content and get a count of tokens used.\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "print (f\"Token count: {len(encoding.encode(content))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a result of something like 60,000 tokens. Which is a huge amount of tokens! But let's continue regardless.\n",
    "\n",
    "Let's setup the OpenAI API to use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in the Prompts section of this workshop, we can provide additional data to an AI model by including that data in the prompt. So, let's construct a prompt that asks for the highest rated movie and then provides the list of movies for the AI to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the highest rated movie from the following list\\n\"\n",
    "query += \"CSV list of movies:\\n\"\n",
    "query += content\n",
    "\n",
    "print (f\"{query[0:3000]} ...[{len(query)} characters]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we've output the first few lines of the prompt and we've printed the overall size (in characters) of the prompt. Now, let's see what happens if we submit that query to the AI.\n",
    "\n",
    "**NOTE:** Don't be surprised to see an error message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "    messages = [{\"role\" : \"assistant\", \"content\" : query}],\n",
    ")\n",
    "\n",
    "print (response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This request will fail. At the end of the output, you should see an error message something like the following:\n",
    "\n",
    "```bash\n",
    "InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 59265 tokens. Please reduce the length of the messages.\n",
    "```\n",
    "\n",
    "You can see quite clearly from the error message that we've exceeded the model's maximum length. Its maximum is **16385** tokens, we've sent a request requiring **59265** tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we've learned about tokens. Your prompts will be broken up and sent to the AI as tokens and all AI models have a maximum token size which you must take care not to exceed. You will also be charged based on the number of tokens that your queries consume."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "In the next lab, we'll look at one of the ways you can take control of the number of tokens your prompts consume by introducing the concept of **Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "ð£ [Embeddings](../02-Embeddings/embeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
