{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - AI Orchestration with Azure Cognitive Search\n",
    "\n",
    "In this lab, we will do a deeper dive around the Azure Cognitive Search vector store and different ways to interact with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Cognitive Search Vector Store in Azure\n",
    "\n",
    "First, we need to create an Azure Cognitive Search service in Azure, which will act as a vector store. We'll use the Azure CLI to do this.\n",
    "\n",
    "**NOTE:** Update **`<INITIALS>`** to make the name unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_GROUP=\"azure-cognitive-search-rg\"\n",
    "LOCATION=\"westeurope\"\n",
    "NAME=\"acs-vectorstore-<INITIALS>\"\n",
    "!az group create --name $RESOURCE_GROUP --location $LOCATION\n",
    "!az search service create -g $RESOURCE_GROUP -n $NAME -l $LOCATION --sku Basic --partition-count 1 --replica-count 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find and update the following values in the `.env` file with the Azure Cognitive Search **endpoint**, **admin key**, and **index name** values. Use the Azure Portal or CLI.\n",
    "\n",
    "```\n",
    "AZURE_COGNITIVE_SEARCH_SERVICE_NAME = \"<YOUR AZURE COGNITIVE SEARCH SERVICE NAME - e.g. cognitive-search-service>\"\n",
    "AZURE_COGNITIVE_SEARCH_ENDPOINT_NAME = \"<YOUR AZURE COGNITIVE SEARCH ENDPOINT NAME - e.g. https://cognitive-search-service.search.windows.net\"\n",
    "AZURE_COGNITIVE_SEARCH_INDEX_NAME = \"<YOUR AZURE COGNITIVE SEARCH INDEX NAME - e.g. cognitive-search-index>\"\n",
    "AZURE_COGNITIVE_SEARCH_API_KEY = \"<YOUR AZURE COGNITIVE SEARCH ADMIN API KEY - e.g. cognitive-search-admin-api-key>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure OpenAI\n",
    "\n",
    "We'll start as usual by defining our Azure OpenAI service API key and endpoint details, specifying the model deployment we want to use and then we'll initiate a connection to the Azure OpenAI service.\n",
    "\n",
    "**NOTE**: As with previous labs, we'll use the values from the `.env` file in the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found OpenAPI Base Endpoint: \" + os.getenv(\"OPENAI_API_BASE\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "embedding_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "acs_service_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\")\n",
    "acs_endpoint_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT_NAME\")\n",
    "acs_index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\")\n",
    "acs_api_key = os.getenv(\"AZURE_COGNITIVE_SEARCH_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the data from the movies.csv file using the Langchain CSV document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Movie Fields in CSV\n",
    "# id,original_language,original_title,popularity,release_date,vote_average,vote_count,genre,overview,revenue,runtime,tagline\n",
    "loader = CSVLoader(file_path='./movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "data = data[1:51] # reduce dataset if you want\n",
    "print('Loaded %s movies' % len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an Azure OpenAI embedding and completion deployments in order to create the vector representation of the movies so we can start asking our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment=embedding_name,\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    embedding_ctx_length=8191,\n",
    "    chunk_size=1000,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "# Create a Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    deployment_name = deployment_name,\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    temperature=0.7,\n",
    "    max_retries=6,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "print('Completed creation of embedding and completion instances.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movies into Azure Cognitive Search\n",
    "\n",
    "Next, we'll create the Azure Cognitive Search index, embed the loaded movies from the CSV file, and upload the data into the newly created index. Depending on the number of movies loaded and rate limiting, this might take a while to do the embeddings so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "# Let's Create the Azure Cognitive Search Index\n",
    "index_client = SearchIndexClient(\n",
    "    acs_endpoint_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "# Movie Fields in CSV\n",
    "# id,original_language,original_title,popularity,release_date,vote_average,vote_count,genre,overview,revenue,runtime,tagline\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"tagline\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"popularity\", type=SearchFieldDataType.Double, sortable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "]\n",
    "\n",
    "# Configure Vector Search Configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"my-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure Semantic Configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"title\"), SemanticField(field_name=\"tagline\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the desired vector search and semantic configurations\n",
    "index = SearchIndex(\n",
    "    name=acs_index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_settings=semantic_settings\n",
    ")\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'The {result.name} index was created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the document structure needed to upload the data into the Azure Cognitive Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the index is created, let's load the documents into it.\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Let's take a quick look at the data structure of the CSVLoader\n",
    "print(data[0])\n",
    "print(data[0].metadata['source'])\n",
    "print(\"----------\")\n",
    "\n",
    "# Generate Document Embeddings for page_content field in the movies CSVLoader dataset using Azure OpenAI\n",
    "items = []\n",
    "for movie in data:\n",
    "    content = movie.page_content\n",
    "    items.append(dict([(\"id\", str(uuid.uuid4())), (\"title\", movie.metadata['source']), (\"content\", content), (\"content_vector\", embeddings.embed_query(content))]))\n",
    "\n",
    "# Print out a sample item to validate the updated data structure.\n",
    "# It should have the id, content, and content_vector values.\n",
    "print(items[0])\n",
    "print(f\"Movie Count: {len(items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will upload the movie documents in the newly created structure to the Azure Cognitive Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload movies to Azure Cognitive Search index.\n",
    "from azure.search.documents.models import Vector\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Insert Text and Embeddings into the Azure Cognitive Search index created.\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "result = search_client.upload_documents(items)\n",
    "print(\"Successfully added documents to Azure Cognitive Search index.\")\n",
    "print(f\"Uploaded {len(data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Azure Cognitive Search\n",
    "\n",
    "Now that we have the movies loaded into Azure Cognitive Search, let's do some different types of searches using the Azure Cognitive Search SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's do a plain vanilla text search, no vectors or embeddings.\n",
    "query = \"What are the best 80s movies I should look at?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# Execute the search\n",
    "results = list(search_client.search(\n",
    "    search_text=query,\n",
    "    include_total_count=True,\n",
    "    top=5\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using only text-based search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over Results\n",
    "# Index Fields - id, content, content_vector\n",
    "for result in results:\n",
    "    print(\"Movie: {}\".format(result[\"content\"]))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do a vector search that uses the embeddings we created and inserted into content_vector field in the index.\n",
    "query = \"What are the best 80s movies I should look at?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# You can see here that we are getting the embedding representation of the query.\n",
    "vector = Vector(\n",
    "    value=embeddings.embed_query(query),\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "\n",
    "# Execute the search\n",
    "results = list(search_client.search(\n",
    "    search_text=\"\",\n",
    "    include_total_count=True,\n",
    "    vectors=[vector],\n",
    "    select=[\"id\", \"content\", \"title\"],\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using only vector-based search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over results and print out the content.\n",
    "for result in results:\n",
    "    print(result[\"title\"])\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that return what you expected? Probably not, let's dig deeper to see why.\n",
    "\n",
    "Let's do the same search again, but this time let's return the **Search Score** so we can see the value returned by the cosine similarity vector store calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again, but this time let's add the relevance score to maybe see why\n",
    "query = \"What are the best 80s movies I should look at?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# You can see here that we are getting the embedding representation of the query.\n",
    "vector = Vector(\n",
    "    value=embeddings.embed_query(query),\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "\n",
    "# Execute the search\n",
    "results = list(search_client.search(\n",
    "    search_text=\"\",\n",
    "    include_total_count=True,\n",
    "    vectors=[vector],\n",
    "    select=[\"id\", \"content\", \"title\"],\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using vector search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over results and print out the id and search score.\n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")\n",
    "    print(f\"Id: {result['title']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the Search Score you will see the relevant ranking of the closest vector match to the query inputted. The lower the score the farther apart the two vectors are. Let's change the search term and see if we can get a higher Search Score which means a higher match and closer vector proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again, but this time let's add the relevance score to maybe see why\n",
    "query = \"Who are the actors in the movie Hidden Figures?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# You can see here that we are getting the embedding representation of the query.\n",
    "vector = Vector(\n",
    "    value=embeddings.embed_query(query),\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "\n",
    "# Execute the search\n",
    "results = list(search_client.search(\n",
    "    search_text=\"\",\n",
    "    include_total_count=True,\n",
    "    vectors=[vector],\n",
    "    select=[\"id\", \"content\", \"title\"],\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using vector search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over results and print out the id and search score.\n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")\n",
    "    print(f\"Id: {result['title']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** As you have seen from the results, different inputs can return different results, it all depends on what data is in the Vector Store. The higher the score the higher the likelihood of a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Searching using Azure Cognitive Search\n",
    "\n",
    "What is Hybrid Search? The search is implemented at the field level, which means you can build queries that include vector fields and searchable text fields. The queries execute in parallel and the results are merged into a single response. Optionally, add semantic search, currently in preview, for even more accuracy with L2 reranking using the same language models that power Bing.\n",
    "\n",
    "**NOTE:** Hybrid Search is a key value proposition of Azure Cognitive Search in comparison to vector only data stores. Click [Hybrid Search](https://learn.microsoft.com/en-us/azure/search/hybrid-search-overview) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "# Let's try our original query again using Hybrid Search (ie. Combination of Text & Vector Search)\n",
    "query = \"What are the best 80s movies I should look at?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# You can see here that we are getting the embedding representation of the query.\n",
    "vector = Vector(\n",
    "    value=embeddings.embed_query(query),\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "\n",
    "# Notice we also fill in the search_text parameter with the query.\n",
    "results = list(search_client.search(\n",
    "    search_text=query,\n",
    "    include_total_count=True,\n",
    "    top=10,\n",
    "    vectors=[vector],\n",
    "    select=[\"id\", \"content\", \"title\"],\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using vector search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over results and print out the id and search score.\n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")\n",
    "    print(result['title'])\n",
    "    print(f\"Hybrid Search Score: {result['@search.score']}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "# Let's try our more specific query again to see the difference in the score returned.\n",
    "query = \"Who are the actors in the movie Hidden Figures?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "\n",
    "# You can see here that we are getting the embedding representation of the query.\n",
    "vector = Vector(\n",
    "    value=embeddings.embed_query(query),\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "\n",
    "# -----\n",
    "# Notice we also fill in the search_text parameter with the query along with the vector.\n",
    "# -----\n",
    "results = list(search_client.search(\n",
    "    search_text=query,\n",
    "    include_total_count=True,\n",
    "    top=10,\n",
    "    vectors=[vector],\n",
    "    select=[\"id\", \"content\", \"title\"],\n",
    "))\n",
    "\n",
    "# Print count of total results.\n",
    "print(f\"Returned {len(results)} results using hybrid search.\")\n",
    "print(\"----------\")\n",
    "# Iterate over results and print out the id and search score.\n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Hybrid Search Score: {result['@search.score']}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it All Together with Retrieval Augmented Generation (RAG) + Langchain (LC)\n",
    "\n",
    "Now that we have our Vector Store setup and data loaded, we are now ready to implement the RAG pattern using AI Orchestration. At a high-level, the following steps are required:\n",
    "1. Ask the question\n",
    "2. Create Prompt Template with inputs\n",
    "3. Get Embedding representation of inputted question\n",
    "4. Use embedded version of the question to search Azure Cognitive Search (ie. The Vector Store)\n",
    "5. Inject the results of the search into the Prompt Template & Execute the Prompt to get the completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RAG using Langchain (LC)\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Setup Langchain\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment=embedding_name,\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    embedding_ctx_length=8191,\n",
    "    chunk_size=1000,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "# Create a Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    deployment_name = deployment_name,\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    temperature=0.7,\n",
    "    max_retries=6,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Ask the question\n",
    "question = \"List the movies about ships on the water.\"\n",
    "\n",
    "# Create a prompt template with variables, note the curly braces\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"original_question\",\"search_results\"],\n",
    "    template=\"\"\"\n",
    "    Question: {original_question}\n",
    "\n",
    "    Do not use any other data.\n",
    "    Only use the movie data below when responding.\n",
    "    {search_results}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Get Embedding for the original question\n",
    "question_embedded=embeddings.embed_query(question)\n",
    "\n",
    "# Search Vector Store\n",
    "search_client = SearchClient(\n",
    "    acs_endpoint_name,\n",
    "    acs_index_name,\n",
    "    AzureKeyCredential(acs_api_key)\n",
    ")\n",
    "vector = Vector(\n",
    "    value=question_embedded,\n",
    "    k=5,\n",
    "    fields=\"content_vector\"\n",
    ")\n",
    "results = list(search_client.search(\n",
    "    search_text=\"\",\n",
    "    include_total_count=True,\n",
    "    vectors=[vector],\n",
    "    select=[\"title\"],\n",
    "))\n",
    "\n",
    "# Build the Prompt and Execute against the Azure OpenAI to get the completion\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "response = chain.run({\"original_question\": question, \"search_results\": results})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "ðŸ“£ [Deploy AI](../../04-deploy-ai/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
