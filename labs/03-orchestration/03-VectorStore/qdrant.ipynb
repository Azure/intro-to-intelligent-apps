{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Qdrant\n",
    "\n",
    "In this lab, we will do a deeper dive around the Qdrant vector store and different ways to interact with it. We'll look at how we can use different search methods to vary the results and how we can use the results with a large language model.\n",
    "\n",
    "We'll start as usual by loading the values from the `.env` file in the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"No file .env found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll use the data from the movies.csv file in this folder, which contains details of around 500 different movies. We'll use a Langchain document loader to load all of the movie data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "# data = data[1:51] # You can uncomment this line to load a smaller subset of the data if you experience issues with token limits or timeouts later in this lab.\n",
    "print('Loaded %s movies' % len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create Azure OpenAI embedding and chat completion deployments. The embeddings instance will be used to create the vector representation of the movies in the loaded CSV file and the chat completion instance will be used to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version = os.getenv(\"OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    model= os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Chat Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Qdrant Server Locally\n",
    "\n",
    "**NOTE**: Please read the following carefully.\n",
    "\n",
    "In previous labs, we ran the Qdrant vector store in memory, so it's contents were lost once Qdrant stopped running. In this lab, we'll run Qdrant to persist data to disk.\n",
    "\n",
    "If you are running this lab in Codespaces or a VS Code local devcontainer, then Qdrant is already running and you can continue to the next section.\n",
    "\n",
    "Otherwise, we need to start a local instance of Qdrant. The easiest way to do this is with Docker. If you don't have Docker on your device, consider running this workshop in a Codespace or a VS Code devcontainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Qdrant Server\n",
    "# !docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)\\qdrantstorage\" qdrant/qdrant\n",
    "# for mac and linux use the following command\n",
    "!docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)/qdrantstorage:/qdrant/storage\" qdrant/qdrant\n",
    "\n",
    "# If you want to stop and cleanup the Qdrant server, uncomment and run the following commands:\n",
    "# !docker stop qdrant\n",
    "# !docker rm qdrant\n",
    "# !rm -rf labs/03-orchestration/03-Qdrant/qdrantstorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movies into Qdrant\n",
    "\n",
    "Now that we have the Qdrant server running and persisting data locally, let's load the movies into the vector store.\n",
    "\n",
    "We'll configure Langchain to use Qdrant as the vector store, embed the loaded documents and store the embeddings in the vector store.\n",
    "\n",
    "**NOTE**: Depending on the number of movies loaded and rate limiting, this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"my_movies\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Qdrant\n",
    "\n",
    "Now we are going to demonstrate how the results of searching a vector store can be affected by using different search methods.\n",
    "\n",
    "The most common method used is a similarity search. This search method aims to return the most similar documents to the query text, using a metric such as cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = qdrant\n",
    "\n",
    "query = \"Can you suggest similar movies to The Matrix?\"\n",
    "\n",
    "query_results = qdrant.similarity_search(query)\n",
    "\n",
    "for doc in query_results:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used method is MMR - Maximal Marginal Relevance - which also executes a similarity search, but the algorithm is geared towards returning a more diverse set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Can you suggest similar movies to The Matrix?\"\n",
    "\n",
    "for doc in retriever.get_relevant_documents(query):\n",
    "    print(doc.metadata['source'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of the above examples, a list of movies will have been returned. Both should be relevant to the query, but the MMR search should return a more diverse set of results. To explain that a little more, the first set of results might, for example, all be from the same genre, whereas the MMR search would return results from a wider range of genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Searching using Langchain Retriever\n",
    "\n",
    "In this part we will use Langchain to search the Qdrant vector store and parse the results via a large language model. This is different from the previous section where we were simply returning the results of a Qdrant search directly.\n",
    "\n",
    "As you will have seen in the **02-Embeddings** lab, we can use the `VectorstoreIndexCreator` to quickly create a vector store index and a retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a Langchain QA (Question Answering) retrieval chain to ask questions about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\", return_source_documents=True)\n",
    "\n",
    "query = \"Do you have a column called popularity?\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we've retrieved a set of search results from the vector store, and then used the LLM to parse the results and provide a natural language answer to the question.\n",
    "\n",
    "We can also see the documents that were retrieved from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could ask a different question, such as \"What is the name of the most popular movie?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"If the popularity score is defined as a higher value being a more popular movie, what is the name of the most popular movie in the data provided?\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you'll see that instead of just returning the search result, we've got a natural language answer to the question.\n",
    "\n",
    "And we can see the set of documents that were retrieved, which will be different to those returned for the previous question because we asked a different question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "📣 [Azure AI Search with Semantic Kernel and C#](../04-ACS/acs-sk-csharp.ipynb)\n",
    "\n",
    "📣 [Azure AI Search with Langchain and Python](../04-ACS/acs-lc-python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
