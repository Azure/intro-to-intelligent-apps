{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Currently not working - please skip this notebook\n",
    "\n",
    "Last part of the notebook will fail. Issue is described here:\n",
    "https://github.com/langchain-ai/langchain/issues/26097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Azure CosmosDB for Mongo DB vCore\n",
    "\n",
    "## Setup, Vectorize and Load Data\n",
    "\n",
    "In this lab, we'll see how to leverage a sample dataset stored in Azure Cosmos DB for MongoDB to ground OpenAI models. We'll do this taking advantage of Azure Cosmos DB for Mongo DB vCore's [vector similarity search](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) functionality. \n",
    "\n",
    "You will need to create at the Azure Portal an M40 cluster by using Azure Cosmos DB for MongoDB vCore. You can create vector indexes on M40 cluster tiers and higher. After the cluster is created add the connection string at the .env file. \n",
    "\n",
    "Let's start by importing the modules we will use. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from time import sleep\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.cache import AzureCosmosDBSemanticCache\n",
    "from langchain_community.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    "    CosmosDBVectorSearchType)\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the values from the `.env` file in the root of the repository and instantiate the mongo and openAI clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Azure OpenAI Endpoint: https://msazuredev.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymongo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "cosmos_conn = os.getenv(\"MONGO_DB_CONNECTION_STRING\")\n",
    "cosmos_database = os.getenv(\"MONGO_DB_database_name\")\n",
    "cosmos_collection = os.getenv(\"MONGO_DB_collection_name\")\n",
    "cosmos_vector_property = os.getenv(\"MONGO_DB_vector_property_name\")\n",
    "cosmos_cache = os.getenv(\"MONGO_DB_cache_collection_name\")\n",
    "\n",
    "storage_file_url = os.getenv(\"storage_file_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/x6qz9qm94xl_v256v8qfl1gm0000gn/T/ipykernel_78714/548968826.py:5: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  cosmos_client = pymongo.MongoClient(cosmos_conn)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Create the MongoDB client\n",
    "cosmos_client = pymongo.MongoClient(cosmos_conn)\n",
    "\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "\tazure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "\tapi_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "\tapi_version=os.getenv(\"OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version = os.getenv(\"OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    model= os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    ")\n",
    "\n",
    "# Create a Chat Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create a collection with a vector index\n",
    "\n",
    "This function takes a database object, a collection name, the name of the document property that will store vectors, and the number of vector dimensions used for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_and_vector_index(database, cosmos_collection, vector_property, embeddings_dimensions):\n",
    "\n",
    "    collection = database[cosmos_collection]\n",
    "\n",
    "    database.command(\n",
    "        {\n",
    "            \"createIndexes\": cosmos_collection,\n",
    "            \"indexes\": [\n",
    "                {\n",
    "                    \"name\": \"VectorSearchIndex\",\n",
    "                    \"key\": {\n",
    "                        vector_property: \"cosmosSearch\"\n",
    "                    },\n",
    "                    \"cosmosSearchOptions\": { \n",
    "                        \"kind\": \"vector-hnsw\", \n",
    "                        \"m\": 16, # default value \n",
    "                        \"efConstruction\": 64, # default value \n",
    "                        \"similarity\": \"COS\", \n",
    "                        \"dimensions\": 1536 # Number of dimensions for vector similarity. The maximum number of supported dimensions is 2000\n",
    "                    } \n",
    "                } \n",
    "            ] \n",
    "        }\n",
    "    )  \n",
    "\n",
    "    return collection\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Database and Collections with Vector Index\n",
    "\n",
    "In this lab, we will create two collections. One that will store the movie data with their embeddings and another to store the promts and the answers with their embeddings to implement semantic cache.\n",
    "\n",
    "â“ What is semantic caching?\n",
    "\n",
    "Caching systems typically store commonly retrieved data for subsequent serving in an optimal manner. In the context of LLMs, semantic cache maintains a cache of previously asked questions and responses, uses similarity measures to retrieve semantically similar queries from the cache and respond with cached responses if a match is found within the threshold for similarity. If cache is not able to return a response, then the answer can be returned from a fresh LLM call.\n",
    "\n",
    "ðŸ‘Œ Benefits of semantic caching:\n",
    "\n",
    "- Cost optimization: Since the responses are served without invoking LLMs, there can be significant cost benefits for caching responses. We have come across use cases where customers have reported 20 â€“ 30 % of the total queries from users can be served by the caching layer.\n",
    "Improvement in latency: LLMs are known to exhibit higher latencies to generate responses. This can be reduced by response caching, to the extent that queries are answered from caching layer and not by invoking LLMs every time.\n",
    "- Scaling: Since questions responded by cache hit do not invoke LLMs, provisioned resources/endpoints are free to answer unseen/newer questions from users. This can be helpful when applications are scaled to handle more users.\n",
    "- Consistency in responses: Since caching layer answers from cached responses, there is no actual generation involved and the same response is provided to queries deemed semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the collection database and drop if it does\n",
    "if cosmos_database in cosmos_client.list_database_names():\n",
    "    cosmos_client.drop_database(cosmos_database)\n",
    "\n",
    "# Create the database \n",
    "database = cosmos_client[cosmos_database]\n",
    "\n",
    "# Create the data collection with vector index\n",
    "collection = create_collection_and_vector_index(database, cosmos_collection, cosmos_vector_property, 1536)\n",
    "\n",
    "# Create the cache collection with vector index\n",
    "cache = create_collection_and_vector_index(database, cosmos_cache, cosmos_vector_property, 1536)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from Azure OpenAI\n",
    "\n",
    "The following function will generate embeddings for a given text. We add retry to handle any throttling due to quota limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=200), stop=stop_after_attempt(20))\n",
    "def generate_embeddings(text):\n",
    "    \n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "        dimensions=1536\n",
    "    )\n",
    "    \n",
    "    embeddings = response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream, vectorize & store\n",
    "\n",
    "In this lab we'll use a subset of the MovieLens dataset. \n",
    "We will stream the data out of blob storage, generate vectors on the *overview* of the json document using the function above, then store it in Azure Cosmos DB for MongoDB collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 documents into collection: 'movie_data'.\n",
      "Inserted 200 documents into collection: 'movie_data'.\n",
      "Inserted 300 documents into collection: 'movie_data'.\n",
      "Inserted 400 documents into collection: 'movie_data'.\n",
      "Inserted 500 documents into collection: 'movie_data'.\n",
      "Data inserted into collection: 'movie_data'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "# open the file and stream the data to ingest\n",
    "stream = urllib.request.urlopen(storage_file_url)\n",
    "\n",
    "counter = 0\n",
    "max_count = 500\n",
    "\n",
    "# iterate through the stream, generate vectors and insert into collection\n",
    "for object in ijson.items(stream, 'item', use_float=True):\n",
    "    if counter >= max_count:\n",
    "        break\n",
    "    \n",
    "    #generate embeddings\n",
    "    vectorArray = generate_embeddings(object['overview'])\n",
    "\n",
    "    #add a vectorArray field to the document that will contain the embeddings \n",
    "    object[cosmos_vector_property] = vectorArray\n",
    "\n",
    "    #insert the document into the collection\n",
    "    collection.insert_one(object)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter % 100 == 0:\n",
    "        print(\"Inserted {} documents into collection: '{}'.\".format(counter, collection.name))\n",
    "        sleep(.5)   # sleep for 0.5 seconds to help avoid rate limiting\n",
    "\n",
    "\n",
    "print(\"Data inserted into collection: '{}'.\\n\".format(collection.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configure Vector Search w/ LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/azure_cosmos_db.py:146: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  client: MongoClient = MongoClient(connection_string, appname=appname)\n"
     ]
    }
   ],
   "source": [
    "cdb = AzureCosmosDBVectorSearch(\n",
    "    collection= cosmos_collection,\n",
    "    embedding=embeddings)\n",
    "\n",
    "vectorstore = cdb.from_connection_string(\n",
    "    connection_string=cosmos_conn,\n",
    "    namespace = cosmos_database + \".\" + cosmos_collection,\n",
    "    embedding = embeddings,\n",
    "    embedding_key = cosmos_vector_property,\n",
    "    text_key = \"overview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG and Semantic Caching with your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's write the prompt template to use for the LLM. We are setting up an AI assistant to help answer questions about our movies dataset. We ask to use the context of the retrieved documents from the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an upbeat AI assistant who is excited to help answer questions about movies.  \n",
    "Use only the context which is the overview of the movies:\n",
    "\n",
    "{context},\n",
    "\n",
    "or this chat history\n",
    "\n",
    "{chat_history},\n",
    "\n",
    "to answer this question. \n",
    "\n",
    "Question: {question}\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\"\"\"\n",
    "chatbot_prompt = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\", \"chat_history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll implement the RAG pattern using LangChain. In LangChain, a retriever is used to augment the prompt with contextual data. In this case, the already established vector store will be used as the `retriever`. This retriever is configured to use a similarity search with a score threshold of 0.2 and to return the top 5 most similar results (k=5).\n",
    "\n",
    "Next we have the `ConversationalRetrievalChain` object that is responsible for managing the retrieval of responses in a conversational context. It is configured with the previously created `retriever` and is set to return the source documents of the retrieved responses. The `combine_docs_chain_kwargs` parameter is set to final prompt of the `ConversationalRetrievalChain`. We add the verbose flag to return the final prompt and see the retrieved documents that will be used for the LLM.  \n",
    "\n",
    "The last part of the chain is to set up the semantic cache. There we need a similarity threshold of 0.99 to match the question asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chain():\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 5, 'score_threshold': 0.2})\n",
    "\n",
    "    sem_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    "    combine_docs_chain_kwargs = {\"prompt\": chatbot_prompt},\n",
    "    verbose=True)\n",
    "\n",
    "    similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "    kind = CosmosDBVectorSearchType.VECTOR_IVF\n",
    "    num_lists = 1\n",
    "    score_threshold = 0.99\n",
    "\n",
    "    sem_cache = AzureCosmosDBSemanticCache(\n",
    "            cosmosdb_connection_string = cosmos_conn,\n",
    "            cosmosdb_client = None,\n",
    "            embedding = embeddings,\n",
    "            database_name = cosmos_database, \n",
    "            collection_name = cosmos_cache,\n",
    "            similarity = similarity_algorithm,\n",
    "            num_lists = num_lists,\n",
    "            kind = kind,\n",
    "            dimensions = 1536,\n",
    "            score_threshold = score_threshold)\n",
    "\n",
    "    set_llm_cache(sem_cache)\n",
    "\n",
    "    return retriever, llm, sem_qa, sem_cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, llm, chain, sem_cache = prepare_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chatbot with a question. This first time, the questions is not yet in cache, so it should take longer as it will send it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain/chains/base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain/chains/base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:158\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    154\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 158\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:395\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_core/retrievers.py:253\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    252\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    256\u001b[0m         result,\n\u001b[1;32m    257\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_core/retrievers.py:246\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py:1042\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m   1040\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1042\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1044\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m   1046\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m   1047\u001b[0m             )\n\u001b[1;32m   1048\u001b[0m         )\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/azure_cosmos_db.py:563\u001b[0m, in \u001b[0;36mAzureCosmosDBVectorSearch.similarity_search\u001b[0;34m(self, query, k, kind, pre_filter, ef_search, score_threshold, with_embedding, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m--> 563\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mef_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mef_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/azure_cosmos_db.py:541\u001b[0m, in \u001b[0;36mAzureCosmosDBVectorSearch.similarity_search_with_score\u001b[0;34m(self, query, k, kind, pre_filter, ef_search, score_threshold, with_embedding)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    532\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     with_embedding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    540\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 541\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_similarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mef_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mef_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/repos/intro-to-intelligent-apps/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/azure_cosmos_db.py:463\u001b[0m, in \u001b[0;36mAzureCosmosDBVectorSearch._similarity_search_with_score\u001b[0;34m(self, embeddings, k, kind, pre_filter, ef_search, score_threshold, with_embedding)\u001b[0m\n\u001b[1;32m    461\u001b[0m document_object_field \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    462\u001b[0m text \u001b[38;5;241m=\u001b[39m document_object_field\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_key)\n\u001b[0;32m--> 463\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mdocument_object_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_embedding:\n\u001b[1;32m    465\u001b[0m     metadata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_key] \u001b[38;5;241m=\u001b[39m document_object_field\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_key\n\u001b[1;32m    467\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'metadata'"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "query = \"Tell me about films with Buzz Lightyear\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask a very similar question you will notice how faster it will be as it will use the semantic cache instead of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"Tell me something about films with Buzz Lightyear\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that in order to answer the question below it will use the context from the documents the chain retrieves from the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Whose spaceship crashed on a desert planet\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "ðŸ“£ [Implement Retrieval Augmented Generation with Azure AI Search as vector store with semantic ranking](./aisearch.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
