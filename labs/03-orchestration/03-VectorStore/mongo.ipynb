{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Azure CosmosDB for Mongo DB vCore\n",
    "\n",
    "## Setup, Vectorize and Load Data\n",
    "\n",
    "In this lab, we'll see how to leverage a sample dataset stored in Azure Cosmos DB for MongoDB to ground OpenAI models. We'll do this taking advantage of Azure Cosmos DB for Mongo DB vCore's [vector similarity search](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) functionality. \n",
    "\n",
    "You will need to create at the Azure Portal an M40 cluster by using Azure Cosmos DB for MongoDB vCore. You can create vector indexes on M40 cluster tiers and higher. After the cluster is created add the connection string at the .env file. \n",
    "\n",
    "Let's start by importing the modules we will use. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from time import sleep\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.cache import AzureCosmosDBSemanticCache\n",
    "from langchain_community.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    "    CosmosDBVectorSearchType)\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the values from the `.env` file in the root of the repository and instantiate the mongo and openAI clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymongo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "cosmos_conn = os.getenv(\"MONGO_DB_CONNECTION_STRING\")\n",
    "cosmos_database = os.getenv(\"MONGO_DB_database_name\")\n",
    "cosmos_collection = os.getenv(\"MONGO_DB_collection_name\")\n",
    "cosmos_vector_property = os.getenv(\"MONGO_DB_vector_property_name\")\n",
    "cosmos_cache = os.getenv(\"MONGO_DB_cache_collection_name\")\n",
    "\n",
    "storage_file_url = os.getenv(\"storage_file_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Create the MongoDB client\n",
    "cosmos_client = pymongo.MongoClient(cosmos_conn)\n",
    "\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "\tazure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "\tapi_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "\tapi_version=os.getenv(\"OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "# Create an Embeddings Instance of Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version = os.getenv(\"OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    model= os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    ")\n",
    "\n",
    "# Create a Chat Completion Instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create a collection with a vector index\n",
    "\n",
    "This function takes a database object, a collection name, the name of the document property that will store vectors, and the number of vector dimensions used for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_and_vector_index(database, cosmos_collection, vector_property, embeddings_dimensions):\n",
    "\n",
    "    collection = database[cosmos_collection]\n",
    "\n",
    "    database.command(\n",
    "        {\n",
    "            \"createIndexes\": cosmos_collection,\n",
    "            \"indexes\": [\n",
    "                {\n",
    "                    \"name\": \"VectorSearchIndex\",\n",
    "                    \"key\": {\n",
    "                        vector_property: \"cosmosSearch\"\n",
    "                    },\n",
    "                    \"cosmosSearchOptions\": { \n",
    "                        \"kind\": \"vector-hnsw\", \n",
    "                        \"m\": 16, # default value \n",
    "                        \"efConstruction\": 64, # default value \n",
    "                        \"similarity\": \"COS\", \n",
    "                        \"dimensions\": 1536 # Number of dimensions for vector similarity. The maximum number of supported dimensions is 2000\n",
    "                    } \n",
    "                } \n",
    "            ] \n",
    "        }\n",
    "    )  \n",
    "\n",
    "    return collection\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Database and Collections with Vector Index\n",
    "\n",
    "In this lab, we will create two collections. One that will store the movie data with their embeddings and another to store the promts and the answers with their embeddings to implement semantic cache.\n",
    "\n",
    "‚ùì What is semantic caching?\n",
    "\n",
    "Caching systems typically store commonly retrieved data for subsequent serving in an optimal manner. In the context of LLMs, semantic cache maintains a cache of previously asked questions and responses, uses similarity measures to retrieve semantically similar queries from the cache and respond with cached responses if a match is found within the threshold for similarity. If cache is not able to return a response, then the answer can be returned from a fresh LLM call.\n",
    "\n",
    "üëå Benefits of semantic caching:\n",
    "\n",
    "- Cost optimization: Since the responses are served without invoking LLMs, there can be significant cost benefits for caching responses. We have come across use cases where customers have reported 20 ‚Äì 30 % of the total queries from users can be served by the caching layer.\n",
    "Improvement in latency: LLMs are known to exhibit higher latencies to generate responses. This can be reduced by response caching, to the extent that queries are answered from caching layer and not by invoking LLMs every time.\n",
    "- Scaling: Since questions responded by cache hit do not invoke LLMs, provisioned resources/endpoints are free to answer unseen/newer questions from users. This can be helpful when applications are scaled to handle more users.\n",
    "- Consistency in responses: Since caching layer answers from cached responses, there is no actual generation involved and the same response is provided to queries deemed semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the collection database and drop if it does\n",
    "if cosmos_database in cosmos_client.list_database_names():\n",
    "    cosmos_client.drop_database(cosmos_database)\n",
    "\n",
    "# Create the database \n",
    "database = cosmos_client[cosmos_database]\n",
    "\n",
    "# Create the data collection with vector index\n",
    "collection = create_collection_and_vector_index(database, cosmos_collection, cosmos_vector_property, 1536)\n",
    "\n",
    "# Create the cache collection with vector index\n",
    "cache = create_collection_and_vector_index(database, cosmos_cache, cosmos_vector_property, 1536)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from Azure OpenAI\n",
    "\n",
    "The following function will generate embeddings for a given text. We add retry to handle any throttling due to quota limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=200), stop=stop_after_attempt(20))\n",
    "def generate_embeddings(text):\n",
    "    \n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "        dimensions=1536\n",
    "    )\n",
    "    \n",
    "    embeddings = response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream, vectorize & store\n",
    "\n",
    "In this lab we'll use a subset of the MovieLens dataset. \n",
    "We will stream the data out of blob storage, generate vectors on the *overview* of the json document using the function above, then store it in Azure Cosmos DB for MongoDB collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "# open the file and stream the data to ingest\n",
    "stream = urllib.request.urlopen(storage_file_url)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# iterate through the stream, generate vectors and insert into collection\n",
    "for object in ijson.items(stream, 'item', use_float=True):\n",
    "\n",
    "    #generate embeddings\n",
    "    vectorArray = generate_embeddings(object['overview'])\n",
    "\n",
    "    #add a vectorArray field to the document that will contain the embeddings \n",
    "    object[cosmos_vector_property] = vectorArray\n",
    "\n",
    "    #insert the document into the collection\n",
    "    collection.insert_one(object)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter % 100 == 0:\n",
    "        print(\"Inserted {} documents into collection: '{}'.\".format(counter, collection.name))\n",
    "        sleep(.5)   # sleep for 0.5 seconds to help avoid rate limiting\n",
    "\n",
    "\n",
    "print(\"Data inserted into collection: '{}'.\\n\".format(collection.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configure Vector Search w/ LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb = AzureCosmosDBVectorSearch(\n",
    "    collection= cosmos_collection,\n",
    "    embedding=embeddings)\n",
    "\n",
    "vectorstore = cdb.from_connection_string(\n",
    "    connection_string=cosmos_conn,\n",
    "    namespace = cosmos_database + \".\" + cosmos_collection,\n",
    "    embedding = embeddings,\n",
    "    embedding_key = cosmos_vector_property,\n",
    "    text_key = \"overview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG and Semantic Caching with your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's write the prompt template to use for the LLM. We are setting up an AI assistant to help answer questions about our movies dataset. We ask to use the context of the retrieved documents from the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an upbeat AI assistant who is excited to help answer questions about movies.  \n",
    "Use only the context which is the overview of the movies:\n",
    "\n",
    "{context},\n",
    "\n",
    "or this chat history\n",
    "\n",
    "{chat_history},\n",
    "\n",
    "to answer this question. \n",
    "\n",
    "Question: {question}\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\"\"\"\n",
    "chatbot_prompt = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\", \"chat_history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll implement the RAG pattern using LangChain. In LangChain, a retriever is used to augment the prompt with contextual data. In this case, the already established vector store will be used as the `retriever`. This retriever is configured to use a similarity search with a score threshold of 0.2 and to return the top 5 most similar results (k=5).\n",
    "\n",
    "Next we have the `ConversationalRetrievalChain` object that is responsible for managing the retrieval of responses in a conversational context. It is configured with the previously created `retriever` and is set to return the source documents of the retrieved responses. The `combine_docs_chain_kwargs` parameter is set to final prompt of the `ConversationalRetrievalChain`. We add the verbose flag to return the final prompt and see the retrieved documents that will be used for the LLM.  \n",
    "\n",
    "The last part of the chain is to set up the semantic cache. There we need a similarity threshold of 0.99 to match the question asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chain():\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 5, 'score_threshold': 0.2})\n",
    "\n",
    "    sem_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    "    combine_docs_chain_kwargs = {\"prompt\": chatbot_prompt},\n",
    "    verbose=True)\n",
    "\n",
    "    similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "    kind = CosmosDBVectorSearchType.VECTOR_IVF\n",
    "    num_lists = 1\n",
    "    score_threshold = 0.99\n",
    "\n",
    "    sem_cache = AzureCosmosDBSemanticCache(\n",
    "            cosmosdb_connection_string = cosmos_conn,\n",
    "            cosmosdb_client = None,\n",
    "            embedding = embeddings,\n",
    "            database_name = cosmos_database, \n",
    "            collection_name = cosmos_cache,\n",
    "            similarity = similarity_algorithm,\n",
    "            num_lists = num_lists,\n",
    "            kind = kind,\n",
    "            dimensions = 1536,\n",
    "            score_threshold = score_threshold)\n",
    "\n",
    "    set_llm_cache(sem_cache)\n",
    "\n",
    "    return retriever, llm, sem_qa, sem_cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, llm, chain, sem_cache = prepare_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chatbot with a question. This first time, the questions is not yet in cache, so it should take longer as it will send it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "query = \"Tell me about films with Buzz Lightyear\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask a very similar question you will notice how faster it will be as it will use the semantic cache instead of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"Tell me something about films with Buzz Lightyear\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that in order to answer the question below it will use the context from the documents the chain retrieves from the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Whose spaceship crashed on a desert planet\"\n",
    "response = chain.invoke({\"question\": query, 'chat_history': [] })\n",
    "print(\"***********************************************************\")\n",
    "print (response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "üì£ [Azure AI Search with Semantic Kernel and C#](../04-ACS/acs-sk-csharp.ipynb)\n",
    "\n",
    "üì£ [Azure AI Search with Semantic Kernel and Python](../04-ACS/acs-sk-python.ipynb)\n",
    "\n",
    "üì£ [Azure AI Search with Langchain and Python](../04-ACS/acs-lc-python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
